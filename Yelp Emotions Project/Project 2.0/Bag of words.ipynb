{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsblo\\Anaconda2\\envs\\p36workshop\\lib\\site-packages\\IPython\\core\\magics\\pylab.py:160: UserWarning: pylab import has clobbered these variables: ['sample']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ujmEBvifdJM6h6RLv4wQIg</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-05-07 04:34:36</td>\n",
       "      <td>1</td>\n",
       "      <td>Q1sbwvVQXV2734tPgoKj4Q</td>\n",
       "      <td>1</td>\n",
       "      <td>Total bill for this horrible service? Over $8G...</td>\n",
       "      <td>6</td>\n",
       "      <td>hG7b0MtEbXx5QzbzE6C_VA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NZnhc2sEQy3RmzKTZnqtwQ</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-01-14 21:30:33</td>\n",
       "      <td>0</td>\n",
       "      <td>GJXCdrto3ASJOqKeVWPi6Q</td>\n",
       "      <td>5</td>\n",
       "      <td>I *adore* Travis at the Hard Rock's new Kelly ...</td>\n",
       "      <td>0</td>\n",
       "      <td>yXQM5uF2jS6es16SJzNHfg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WTqjgwHlXbSFevF32_DJVw</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-11-09 20:09:03</td>\n",
       "      <td>0</td>\n",
       "      <td>2TzJjDVDEuAW6MR5Vuc1ug</td>\n",
       "      <td>5</td>\n",
       "      <td>I have to say that this office really has it t...</td>\n",
       "      <td>3</td>\n",
       "      <td>n6-Gk65cPZL6Uz8qRm3NYw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ikCg8xy5JIg_NGPx-MSIDA</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-01-09 20:56:38</td>\n",
       "      <td>0</td>\n",
       "      <td>yi0R0Ugj_xUx_Nek0-_Qig</td>\n",
       "      <td>5</td>\n",
       "      <td>Went in for a lunch. Steak sandwich was delici...</td>\n",
       "      <td>0</td>\n",
       "      <td>dacAIZ6fTM6mqwW5uxkskg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b1b1eb3uo-w561D0ZfCEiQ</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-01-30 23:07:38</td>\n",
       "      <td>0</td>\n",
       "      <td>11a8sVPMUFtaC7_ABRkmtw</td>\n",
       "      <td>1</td>\n",
       "      <td>Today was my second out of three sessions I ha...</td>\n",
       "      <td>7</td>\n",
       "      <td>ssoyf2_x0EQMed6fgHeMyQ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id  cool                date  funny  \\\n",
       "0  ujmEBvifdJM6h6RLv4wQIg     0 2013-05-07 04:34:36      1   \n",
       "1  NZnhc2sEQy3RmzKTZnqtwQ     0 2017-01-14 21:30:33      0   \n",
       "2  WTqjgwHlXbSFevF32_DJVw     0 2016-11-09 20:09:03      0   \n",
       "3  ikCg8xy5JIg_NGPx-MSIDA     0 2018-01-09 20:56:38      0   \n",
       "4  b1b1eb3uo-w561D0ZfCEiQ     0 2018-01-30 23:07:38      0   \n",
       "\n",
       "                review_id  stars  \\\n",
       "0  Q1sbwvVQXV2734tPgoKj4Q      1   \n",
       "1  GJXCdrto3ASJOqKeVWPi6Q      5   \n",
       "2  2TzJjDVDEuAW6MR5Vuc1ug      5   \n",
       "3  yi0R0Ugj_xUx_Nek0-_Qig      5   \n",
       "4  11a8sVPMUFtaC7_ABRkmtw      1   \n",
       "\n",
       "                                                text  useful  \\\n",
       "0  Total bill for this horrible service? Over $8G...       6   \n",
       "1  I *adore* Travis at the Hard Rock's new Kelly ...       0   \n",
       "2  I have to say that this office really has it t...       3   \n",
       "3  Went in for a lunch. Steak sandwich was delici...       0   \n",
       "4  Today was my second out of three sessions I ha...       7   \n",
       "\n",
       "                  user_id  \n",
       "0  hG7b0MtEbXx5QzbzE6C_VA  \n",
       "1  yXQM5uF2jS6es16SJzNHfg  \n",
       "2  n6-Gk65cPZL6Uz8qRm3NYw  \n",
       "3  dacAIZ6fTM6mqwW5uxkskg  \n",
       "4  ssoyf2_x0EQMed6fgHeMyQ  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas import Series, DataFrame\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from random import sample\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from itertools import chain\n",
    "nltk.download('words')\n",
    "%pylab inline\n",
    "\n",
    "df = pd.read_json('yelp_academic_dataset_review_cleaned.json', lines=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102292, 9)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10229, 9)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sample(frac=0.1, replace=False, random_state=1)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 22.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# tokenization = split sentences into word strings\n",
    "df['text_list'] = df['text'].map(nltk.word_tokenize)\n",
    "df['tokens'] = df['text_list'].map(lambda x: set(x)) #set is like a list but removes duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 13.4 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "      <th>text_list</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29369</th>\n",
       "      <td>kPR47uxGqmQNIQkIZr2_Tg</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-06-12 04:28:37</td>\n",
       "      <td>0</td>\n",
       "      <td>kJa4M0WsQaGwoMDSP0c-hw</td>\n",
       "      <td>5</td>\n",
       "      <td>Friendly staff and generous portions of great ...</td>\n",
       "      <td>1</td>\n",
       "      <td>sizanx4RIwRC3BfhQv8KOg</td>\n",
       "      <td>[Friendly, staff, and, generous, portions, of,...</td>\n",
       "      <td>{generous, staff, and, food, !, u, Friday, of,...</td>\n",
       "      <td>{generous, food, staff, !, and, rock, u, Frida...</td>\n",
       "      <td>[generous, food, staff, rock, u, friday, frien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80459</th>\n",
       "      <td>qHQPvp6pZ75fB63kOKUPqg</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-08-13 19:10:43</td>\n",
       "      <td>1</td>\n",
       "      <td>8qst2v9HE8tV7B3IFvtSIA</td>\n",
       "      <td>5</td>\n",
       "      <td>Job well done!! Thank you Rick for making our ...</td>\n",
       "      <td>2</td>\n",
       "      <td>DXP3K7wG_nu9jYPzQ8xVFA</td>\n",
       "      <td>[Job, well, done, !, !, Thank, you, Rick, for,...</td>\n",
       "      <td>{the, for, and, look, a, our, ., on, much, was...</td>\n",
       "      <td>{the, for, and, look, a, our, ., on, yearly, c...</td>\n",
       "      <td>[look, yearly, carpet, sure, thank, clean, bes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21659</th>\n",
       "      <td>vhIJ91MDgUuk4Cr9Kpj1Nw</td>\n",
       "      <td>0</td>\n",
       "      <td>2012-04-29 10:58:50</td>\n",
       "      <td>0</td>\n",
       "      <td>DYAIyC476YkouqJgXuUv_A</td>\n",
       "      <td>4</td>\n",
       "      <td>I'm a big fan of Jimmy John's. This location i...</td>\n",
       "      <td>1</td>\n",
       "      <td>OTRxfMCtfrbUemIigxCaOg</td>\n",
       "      <td>[I, 'm, a, big, fan, of, Jimmy, John, 's, ., T...</td>\n",
       "      <td>{and, get, unfriendly, bacon, on, service, loc...</td>\n",
       "      <td>{and, get, unfriendly, bacon, on, service, loc...</td>\n",
       "      <td>[get, unfriendly, bacon, service, location, mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69369</th>\n",
       "      <td>oH4iqq4kjJfXpHCgB9G1sw</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-11-14 21:10:14</td>\n",
       "      <td>0</td>\n",
       "      <td>vCJS9ok_G3OfuxCtT3UNYg</td>\n",
       "      <td>4</td>\n",
       "      <td>Well Spa was surprisingly posh and I loved it!...</td>\n",
       "      <td>0</td>\n",
       "      <td>d84GXn6uhT1qyDEziAmR8Q</td>\n",
       "      <td>[Well, Spa, was, surprisingly, posh, and, I, l...</td>\n",
       "      <td>{the, and, massage, full, posh, loved, ., it, ...</td>\n",
       "      <td>{the, and, massage, amenity, full, posh, loved...</td>\n",
       "      <td>[massage, amenity, full, posh, loved, great, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93624</th>\n",
       "      <td>kPR47uxGqmQNIQkIZr2_Tg</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-07-17 22:52:49</td>\n",
       "      <td>0</td>\n",
       "      <td>rMpvc3Hua7qRtDph8_pnpw</td>\n",
       "      <td>5</td>\n",
       "      <td>Wanted to try this place for a while and it di...</td>\n",
       "      <td>0</td>\n",
       "      <td>Ixaz7mLwzGQGg_W2yfBGqg</td>\n",
       "      <td>[Wanted, to, try, this, place, for, a, while, ...</td>\n",
       "      <td>{for, and, food, banana, a, ordering, -, our, ...</td>\n",
       "      <td>{for, food, banana, and, dessert, a, ordering,...</td>\n",
       "      <td>[food, banana, dessert, ordering, help, delici...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  business_id  cool                date  funny  \\\n",
       "29369  kPR47uxGqmQNIQkIZr2_Tg     0 2016-06-12 04:28:37      0   \n",
       "80459  qHQPvp6pZ75fB63kOKUPqg     0 2014-08-13 19:10:43      1   \n",
       "21659  vhIJ91MDgUuk4Cr9Kpj1Nw     0 2012-04-29 10:58:50      0   \n",
       "69369  oH4iqq4kjJfXpHCgB9G1sw     0 2013-11-14 21:10:14      0   \n",
       "93624  kPR47uxGqmQNIQkIZr2_Tg     0 2015-07-17 22:52:49      0   \n",
       "\n",
       "                    review_id  stars  \\\n",
       "29369  kJa4M0WsQaGwoMDSP0c-hw      5   \n",
       "80459  8qst2v9HE8tV7B3IFvtSIA      5   \n",
       "21659  DYAIyC476YkouqJgXuUv_A      4   \n",
       "69369  vCJS9ok_G3OfuxCtT3UNYg      4   \n",
       "93624  rMpvc3Hua7qRtDph8_pnpw      5   \n",
       "\n",
       "                                                    text  useful  \\\n",
       "29369  Friendly staff and generous portions of great ...       1   \n",
       "80459  Job well done!! Thank you Rick for making our ...       2   \n",
       "21659  I'm a big fan of Jimmy John's. This location i...       1   \n",
       "69369  Well Spa was surprisingly posh and I loved it!...       0   \n",
       "93624  Wanted to try this place for a while and it di...       0   \n",
       "\n",
       "                      user_id  \\\n",
       "29369  sizanx4RIwRC3BfhQv8KOg   \n",
       "80459  DXP3K7wG_nu9jYPzQ8xVFA   \n",
       "21659  OTRxfMCtfrbUemIigxCaOg   \n",
       "69369  d84GXn6uhT1qyDEziAmR8Q   \n",
       "93624  Ixaz7mLwzGQGg_W2yfBGqg   \n",
       "\n",
       "                                               text_list  \\\n",
       "29369  [Friendly, staff, and, generous, portions, of,...   \n",
       "80459  [Job, well, done, !, !, Thank, you, Rick, for,...   \n",
       "21659  [I, 'm, a, big, fan, of, Jimmy, John, 's, ., T...   \n",
       "69369  [Well, Spa, was, surprisingly, posh, and, I, l...   \n",
       "93624  [Wanted, to, try, this, place, for, a, while, ...   \n",
       "\n",
       "                                                  tokens  \\\n",
       "29369  {generous, staff, and, food, !, u, Friday, of,...   \n",
       "80459  {the, for, and, look, a, our, ., on, much, was...   \n",
       "21659  {and, get, unfriendly, bacon, on, service, loc...   \n",
       "69369  {the, and, massage, full, posh, loved, ., it, ...   \n",
       "93624  {for, and, food, banana, a, ordering, -, our, ...   \n",
       "\n",
       "                                              lemmatized  \\\n",
       "29369  {generous, food, staff, !, and, rock, u, Frida...   \n",
       "80459  {the, for, and, look, a, our, ., on, yearly, c...   \n",
       "21659  {and, get, unfriendly, bacon, on, service, loc...   \n",
       "69369  {the, and, massage, amenity, full, posh, loved...   \n",
       "93624  {for, food, banana, and, dessert, a, ordering,...   \n",
       "\n",
       "                                               processed  \n",
       "29369  [generous, food, staff, rock, u, friday, frien...  \n",
       "80459  [look, yearly, carpet, sure, thank, clean, bes...  \n",
       "21659  [get, unfriendly, bacon, service, location, mi...  \n",
       "69369  [massage, amenity, full, posh, loved, great, p...  \n",
       "93624  [food, banana, dessert, ordering, help, delici...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# lemmatization = converting a word to its base form, different from stemming\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df['lemmatized'] = df['tokens'].map(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "df['lemmatized'] = df['lemmatized'].map(lambda x: set(x))\n",
    "\n",
    "# remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['processed'] = df['lemmatized'].map(lambda x: [word for word in x if word.lower() not in stop_words])\n",
    "\n",
    "# remove punctuations\n",
    "punc = \"!\\\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~...\"\n",
    "df['processed'] = df['processed'].map(lambda x: [word for word in x if word.lower() not in punc])\n",
    "\n",
    "# remove some other stuff and return lower case\n",
    "others = [\"''\", \"``\", \"n't\", \"l\", \"'m\", \"'s\"]\n",
    "df['processed'] = df['processed'].map(lambda x: [word.lower() for word in x if word.lower() not in others])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = []\n",
    "\n",
    "def extract_words(text):\n",
    "    for word in text:\n",
    "        vocab_list.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29369     None\n",
       "80459     None\n",
       "21659     None\n",
       "69369     None\n",
       "93624     None\n",
       "2576      None\n",
       "79062     None\n",
       "34064     None\n",
       "12698     None\n",
       "101325    None\n",
       "64011     None\n",
       "91350     None\n",
       "69334     None\n",
       "54226     None\n",
       "33096     None\n",
       "95739     None\n",
       "88453     None\n",
       "26324     None\n",
       "17801     None\n",
       "27883     None\n",
       "96503     None\n",
       "17910     None\n",
       "67133     None\n",
       "14981     None\n",
       "65232     None\n",
       "41526     None\n",
       "99308     None\n",
       "6087      None\n",
       "52792     None\n",
       "86427     None\n",
       "          ... \n",
       "13650     None\n",
       "60924     None\n",
       "80832     None\n",
       "55219     None\n",
       "42542     None\n",
       "80386     None\n",
       "18198     None\n",
       "30170     None\n",
       "378       None\n",
       "97323     None\n",
       "93812     None\n",
       "39        None\n",
       "85022     None\n",
       "99682     None\n",
       "15459     None\n",
       "57417     None\n",
       "1298      None\n",
       "76795     None\n",
       "24395     None\n",
       "61577     None\n",
       "31194     None\n",
       "44875     None\n",
       "101962    None\n",
       "10084     None\n",
       "91058     None\n",
       "14409     None\n",
       "101424    None\n",
       "15957     None\n",
       "38912     None\n",
       "85374     None\n",
       "Name: processed, Length: 10229, dtype: object"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"processed\"].map(extract_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = list(set(vocab_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30284"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import words\n",
    "\n",
    "empty_list = []\n",
    "\n",
    "for each in vocab_list:\n",
    "    if each in words.words():\n",
    "        empty_list.append(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12263"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(empty_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "L = empty_list\n",
    "\n",
    "f = lambda x: Counter([y for y in x if y in L])\n",
    "\n",
    "df['bag_vector'] = (pd.DataFrame(df['processed'].apply(f).values.tolist())\n",
    "               .fillna(0)\n",
    "               .astype(int)\n",
    "               .reindex(columns=L)\n",
    "               .values\n",
    "               .tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "      <th>text_list</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>processed</th>\n",
       "      <th>bag_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29369</th>\n",
       "      <td>kPR47uxGqmQNIQkIZr2_Tg</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-06-12 04:28:37</td>\n",
       "      <td>0</td>\n",
       "      <td>kJa4M0WsQaGwoMDSP0c-hw</td>\n",
       "      <td>5</td>\n",
       "      <td>Friendly staff and generous portions of great ...</td>\n",
       "      <td>1</td>\n",
       "      <td>sizanx4RIwRC3BfhQv8KOg</td>\n",
       "      <td>[Friendly, staff, and, generous, portions, of,...</td>\n",
       "      <td>{generous, staff, and, food, !, u, Friday, of,...</td>\n",
       "      <td>{generous, food, staff, !, and, rock, u, Frida...</td>\n",
       "      <td>[generous, food, staff, rock, u, friday, frien...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80459</th>\n",
       "      <td>qHQPvp6pZ75fB63kOKUPqg</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-08-13 19:10:43</td>\n",
       "      <td>1</td>\n",
       "      <td>8qst2v9HE8tV7B3IFvtSIA</td>\n",
       "      <td>5</td>\n",
       "      <td>Job well done!! Thank you Rick for making our ...</td>\n",
       "      <td>2</td>\n",
       "      <td>DXP3K7wG_nu9jYPzQ8xVFA</td>\n",
       "      <td>[Job, well, done, !, !, Thank, you, Rick, for,...</td>\n",
       "      <td>{the, for, and, look, a, our, ., on, much, was...</td>\n",
       "      <td>{the, for, and, look, a, our, ., on, yearly, c...</td>\n",
       "      <td>[look, yearly, carpet, sure, thank, clean, bes...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21659</th>\n",
       "      <td>vhIJ91MDgUuk4Cr9Kpj1Nw</td>\n",
       "      <td>0</td>\n",
       "      <td>2012-04-29 10:58:50</td>\n",
       "      <td>0</td>\n",
       "      <td>DYAIyC476YkouqJgXuUv_A</td>\n",
       "      <td>4</td>\n",
       "      <td>I'm a big fan of Jimmy John's. This location i...</td>\n",
       "      <td>1</td>\n",
       "      <td>OTRxfMCtfrbUemIigxCaOg</td>\n",
       "      <td>[I, 'm, a, big, fan, of, Jimmy, John, 's, ., T...</td>\n",
       "      <td>{and, get, unfriendly, bacon, on, service, loc...</td>\n",
       "      <td>{and, get, unfriendly, bacon, on, service, loc...</td>\n",
       "      <td>[get, unfriendly, bacon, service, location, mi...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69369</th>\n",
       "      <td>oH4iqq4kjJfXpHCgB9G1sw</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-11-14 21:10:14</td>\n",
       "      <td>0</td>\n",
       "      <td>vCJS9ok_G3OfuxCtT3UNYg</td>\n",
       "      <td>4</td>\n",
       "      <td>Well Spa was surprisingly posh and I loved it!...</td>\n",
       "      <td>0</td>\n",
       "      <td>d84GXn6uhT1qyDEziAmR8Q</td>\n",
       "      <td>[Well, Spa, was, surprisingly, posh, and, I, l...</td>\n",
       "      <td>{the, and, massage, full, posh, loved, ., it, ...</td>\n",
       "      <td>{the, and, massage, amenity, full, posh, loved...</td>\n",
       "      <td>[massage, amenity, full, posh, loved, great, p...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93624</th>\n",
       "      <td>kPR47uxGqmQNIQkIZr2_Tg</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-07-17 22:52:49</td>\n",
       "      <td>0</td>\n",
       "      <td>rMpvc3Hua7qRtDph8_pnpw</td>\n",
       "      <td>5</td>\n",
       "      <td>Wanted to try this place for a while and it di...</td>\n",
       "      <td>0</td>\n",
       "      <td>Ixaz7mLwzGQGg_W2yfBGqg</td>\n",
       "      <td>[Wanted, to, try, this, place, for, a, while, ...</td>\n",
       "      <td>{for, and, food, banana, a, ordering, -, our, ...</td>\n",
       "      <td>{for, food, banana, and, dessert, a, ordering,...</td>\n",
       "      <td>[food, banana, dessert, ordering, help, delici...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  business_id  cool                date  funny  \\\n",
       "29369  kPR47uxGqmQNIQkIZr2_Tg     0 2016-06-12 04:28:37      0   \n",
       "80459  qHQPvp6pZ75fB63kOKUPqg     0 2014-08-13 19:10:43      1   \n",
       "21659  vhIJ91MDgUuk4Cr9Kpj1Nw     0 2012-04-29 10:58:50      0   \n",
       "69369  oH4iqq4kjJfXpHCgB9G1sw     0 2013-11-14 21:10:14      0   \n",
       "93624  kPR47uxGqmQNIQkIZr2_Tg     0 2015-07-17 22:52:49      0   \n",
       "\n",
       "                    review_id  stars  \\\n",
       "29369  kJa4M0WsQaGwoMDSP0c-hw      5   \n",
       "80459  8qst2v9HE8tV7B3IFvtSIA      5   \n",
       "21659  DYAIyC476YkouqJgXuUv_A      4   \n",
       "69369  vCJS9ok_G3OfuxCtT3UNYg      4   \n",
       "93624  rMpvc3Hua7qRtDph8_pnpw      5   \n",
       "\n",
       "                                                    text  useful  \\\n",
       "29369  Friendly staff and generous portions of great ...       1   \n",
       "80459  Job well done!! Thank you Rick for making our ...       2   \n",
       "21659  I'm a big fan of Jimmy John's. This location i...       1   \n",
       "69369  Well Spa was surprisingly posh and I loved it!...       0   \n",
       "93624  Wanted to try this place for a while and it di...       0   \n",
       "\n",
       "                      user_id  \\\n",
       "29369  sizanx4RIwRC3BfhQv8KOg   \n",
       "80459  DXP3K7wG_nu9jYPzQ8xVFA   \n",
       "21659  OTRxfMCtfrbUemIigxCaOg   \n",
       "69369  d84GXn6uhT1qyDEziAmR8Q   \n",
       "93624  Ixaz7mLwzGQGg_W2yfBGqg   \n",
       "\n",
       "                                               text_list  \\\n",
       "29369  [Friendly, staff, and, generous, portions, of,...   \n",
       "80459  [Job, well, done, !, !, Thank, you, Rick, for,...   \n",
       "21659  [I, 'm, a, big, fan, of, Jimmy, John, 's, ., T...   \n",
       "69369  [Well, Spa, was, surprisingly, posh, and, I, l...   \n",
       "93624  [Wanted, to, try, this, place, for, a, while, ...   \n",
       "\n",
       "                                                  tokens  \\\n",
       "29369  {generous, staff, and, food, !, u, Friday, of,...   \n",
       "80459  {the, for, and, look, a, our, ., on, much, was...   \n",
       "21659  {and, get, unfriendly, bacon, on, service, loc...   \n",
       "69369  {the, and, massage, full, posh, loved, ., it, ...   \n",
       "93624  {for, and, food, banana, a, ordering, -, our, ...   \n",
       "\n",
       "                                              lemmatized  \\\n",
       "29369  {generous, food, staff, !, and, rock, u, Frida...   \n",
       "80459  {the, for, and, look, a, our, ., on, yearly, c...   \n",
       "21659  {and, get, unfriendly, bacon, on, service, loc...   \n",
       "69369  {the, and, massage, amenity, full, posh, loved...   \n",
       "93624  {for, food, banana, and, dessert, a, ordering,...   \n",
       "\n",
       "                                               processed  \\\n",
       "29369  [generous, food, staff, rock, u, friday, frien...   \n",
       "80459  [look, yearly, carpet, sure, thank, clean, bes...   \n",
       "21659  [get, unfriendly, bacon, service, location, mi...   \n",
       "69369  [massage, amenity, full, posh, loved, great, p...   \n",
       "93624  [food, banana, dessert, ordering, help, delici...   \n",
       "\n",
       "                                              bag_vector  \n",
       "29369  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "80459  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "21659  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "69369  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "93624  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0008154611432765229"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sanity check\n",
    "def Average(lst): \n",
    "    return sum(lst) / len(lst)\n",
    "\n",
    "first_vect = df[\"bag_vector\"].iloc[0]\n",
    "Average(first_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12263, 1)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_df = pd.DataFrame(L) \n",
    "vocab_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"bag_of_words.csv\")\n",
    "vocab_df.to_csv(\"vocab_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p36workshop",
   "language": "python",
   "name": "p36workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
