{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Q1sbwvVQXV2734tPgoKj4Q</td>\n",
       "      <td>hG7b0MtEbXx5QzbzE6C_VA</td>\n",
       "      <td>ujmEBvifdJM6h6RLv4wQIg</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Total bill for this horrible service? Over $8G...</td>\n",
       "      <td>2013-05-07 04:34:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>GJXCdrto3ASJOqKeVWPi6Q</td>\n",
       "      <td>yXQM5uF2jS6es16SJzNHfg</td>\n",
       "      <td>NZnhc2sEQy3RmzKTZnqtwQ</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I *adore* Travis at the Hard Rock's new Kelly ...</td>\n",
       "      <td>2017-01-14 21:30:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2TzJjDVDEuAW6MR5Vuc1ug</td>\n",
       "      <td>n6-Gk65cPZL6Uz8qRm3NYw</td>\n",
       "      <td>WTqjgwHlXbSFevF32_DJVw</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I have to say that this office really has it t...</td>\n",
       "      <td>2016-11-09 20:09:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>yi0R0Ugj_xUx_Nek0-_Qig</td>\n",
       "      <td>dacAIZ6fTM6mqwW5uxkskg</td>\n",
       "      <td>ikCg8xy5JIg_NGPx-MSIDA</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Went in for a lunch. Steak sandwich was delici...</td>\n",
       "      <td>2018-01-09 20:56:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>11a8sVPMUFtaC7_ABRkmtw</td>\n",
       "      <td>ssoyf2_x0EQMed6fgHeMyQ</td>\n",
       "      <td>b1b1eb3uo-w561D0ZfCEiQ</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Today was my second out of three sessions I ha...</td>\n",
       "      <td>2018-01-30 23:07:38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                review_id                 user_id             business_id  \\\n",
       "0  Q1sbwvVQXV2734tPgoKj4Q  hG7b0MtEbXx5QzbzE6C_VA  ujmEBvifdJM6h6RLv4wQIg   \n",
       "1  GJXCdrto3ASJOqKeVWPi6Q  yXQM5uF2jS6es16SJzNHfg  NZnhc2sEQy3RmzKTZnqtwQ   \n",
       "2  2TzJjDVDEuAW6MR5Vuc1ug  n6-Gk65cPZL6Uz8qRm3NYw  WTqjgwHlXbSFevF32_DJVw   \n",
       "3  yi0R0Ugj_xUx_Nek0-_Qig  dacAIZ6fTM6mqwW5uxkskg  ikCg8xy5JIg_NGPx-MSIDA   \n",
       "4  11a8sVPMUFtaC7_ABRkmtw  ssoyf2_x0EQMed6fgHeMyQ  b1b1eb3uo-w561D0ZfCEiQ   \n",
       "\n",
       "   stars  useful  funny  cool  \\\n",
       "0      1       6      1     0   \n",
       "1      5       0      0     0   \n",
       "2      5       3      0     0   \n",
       "3      5       0      0     0   \n",
       "4      1       7      0     0   \n",
       "\n",
       "                                                text                date  \n",
       "0  Total bill for this horrible service? Over $8G... 2013-05-07 04:34:36  \n",
       "1  I *adore* Travis at the Hard Rock's new Kelly ... 2017-01-14 21:30:33  \n",
       "2  I have to say that this office really has it t... 2016-11-09 20:09:03  \n",
       "3  Went in for a lunch. Steak sandwich was delici... 2018-01-09 20:56:38  \n",
       "4  Today was my second out of three sessions I ha... 2018-01-30 23:07:38  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "reviews_full = pd.read_json('reviews.json', lines=True)\n",
    "reviews_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>kJa4M0WsQaGwoMDSP0c-hw</td>\n",
       "      <td>sizanx4RIwRC3BfhQv8KOg</td>\n",
       "      <td>kPR47uxGqmQNIQkIZr2_Tg</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Friendly staff and generous portions of great ...</td>\n",
       "      <td>2016-06-12 04:28:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>8qst2v9HE8tV7B3IFvtSIA</td>\n",
       "      <td>DXP3K7wG_nu9jYPzQ8xVFA</td>\n",
       "      <td>qHQPvp6pZ75fB63kOKUPqg</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Job well done!! Thank you Rick for making our ...</td>\n",
       "      <td>2014-08-13 19:10:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>DYAIyC476YkouqJgXuUv_A</td>\n",
       "      <td>OTRxfMCtfrbUemIigxCaOg</td>\n",
       "      <td>vhIJ91MDgUuk4Cr9Kpj1Nw</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I'm a big fan of Jimmy John's. This location i...</td>\n",
       "      <td>2012-04-29 10:58:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>vCJS9ok_G3OfuxCtT3UNYg</td>\n",
       "      <td>d84GXn6uhT1qyDEziAmR8Q</td>\n",
       "      <td>oH4iqq4kjJfXpHCgB9G1sw</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Well Spa was surprisingly posh and I loved it!...</td>\n",
       "      <td>2013-11-14 21:10:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>rMpvc3Hua7qRtDph8_pnpw</td>\n",
       "      <td>Ixaz7mLwzGQGg_W2yfBGqg</td>\n",
       "      <td>kPR47uxGqmQNIQkIZr2_Tg</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Wanted to try this place for a while and it di...</td>\n",
       "      <td>2015-07-17 22:52:49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                review_id                 user_id             business_id  \\\n",
       "0  kJa4M0WsQaGwoMDSP0c-hw  sizanx4RIwRC3BfhQv8KOg  kPR47uxGqmQNIQkIZr2_Tg   \n",
       "1  8qst2v9HE8tV7B3IFvtSIA  DXP3K7wG_nu9jYPzQ8xVFA  qHQPvp6pZ75fB63kOKUPqg   \n",
       "2  DYAIyC476YkouqJgXuUv_A  OTRxfMCtfrbUemIigxCaOg  vhIJ91MDgUuk4Cr9Kpj1Nw   \n",
       "3  vCJS9ok_G3OfuxCtT3UNYg  d84GXn6uhT1qyDEziAmR8Q  oH4iqq4kjJfXpHCgB9G1sw   \n",
       "4  rMpvc3Hua7qRtDph8_pnpw  Ixaz7mLwzGQGg_W2yfBGqg  kPR47uxGqmQNIQkIZr2_Tg   \n",
       "\n",
       "   stars  useful  funny  cool  \\\n",
       "0      5       1      0     0   \n",
       "1      5       2      1     0   \n",
       "2      4       1      0     0   \n",
       "3      4       0      0     0   \n",
       "4      5       0      0     0   \n",
       "\n",
       "                                                text                date  \n",
       "0  Friendly staff and generous portions of great ... 2016-06-12 04:28:37  \n",
       "1  Job well done!! Thank you Rick for making our ... 2014-08-13 19:10:43  \n",
       "2  I'm a big fan of Jimmy John's. This location i... 2012-04-29 10:58:50  \n",
       "3  Well Spa was surprisingly posh and I loved it!... 2013-11-14 21:10:14  \n",
       "4  Wanted to try this place for a while and it di... 2015-07-17 22:52:49  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = reviews_full.sample(frac=0.1, replace=False, random_state=1)\n",
    "reviews = reviews.reset_index(drop=True)\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 16.9 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>kJa4M0WsQaGwoMDSP0c-hw</td>\n",
       "      <td>sizanx4RIwRC3BfhQv8KOg</td>\n",
       "      <td>kPR47uxGqmQNIQkIZr2_Tg</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Friendly staff and generous portions of great ...</td>\n",
       "      <td>2016-06-12 04:28:37</td>\n",
       "      <td>{and, generous, eat, !, can, portions, ., u, g...</td>\n",
       "      <td>{portion, staff, and, of, generous, eat, can, ...</td>\n",
       "      <td>[portion, staff, generous, eat, u, great, fish...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>8qst2v9HE8tV7B3IFvtSIA</td>\n",
       "      <td>DXP3K7wG_nu9jYPzQ8xVFA</td>\n",
       "      <td>qHQPvp6pZ75fB63kOKUPqg</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Job well done!! Thank you Rick for making our ...</td>\n",
       "      <td>2014-08-13 19:10:43</td>\n",
       "      <td>{look, extremely, was, to, excellence, Job, ar...</td>\n",
       "      <td>{look, extremely, to, excellence, Job, arrived...</td>\n",
       "      <td>[look, extremely, excellence, job, arrived, st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>DYAIyC476YkouqJgXuUv_A</td>\n",
       "      <td>OTRxfMCtfrbUemIigxCaOg</td>\n",
       "      <td>vhIJ91MDgUuk4Cr9Kpj1Nw</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I'm a big fan of Jimmy John's. This location i...</td>\n",
       "      <td>2012-04-29 10:58:50</td>\n",
       "      <td>{rush, 's, mile, the, bacon, order, service, a...</td>\n",
       "      <td>{rush, 's, mile, bacon, the, doe, order, servi...</td>\n",
       "      <td>[rush, mile, bacon, doe, order, service, alway...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>vCJS9ok_G3OfuxCtT3UNYg</td>\n",
       "      <td>d84GXn6uhT1qyDEziAmR8Q</td>\n",
       "      <td>oH4iqq4kjJfXpHCgB9G1sw</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Well Spa was surprisingly posh and I loved it!...</td>\n",
       "      <td>2013-11-14 21:10:14</td>\n",
       "      <td>{was, to, the, full, any, Spa, Great, loungers...</td>\n",
       "      <td>{to, the, full, any, Spa, Great, had, most, ma...</td>\n",
       "      <td>[full, spa, great, massage, nice, lounger, 've...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>rMpvc3Hua7qRtDph8_pnpw</td>\n",
       "      <td>Ixaz7mLwzGQGg_W2yfBGqg</td>\n",
       "      <td>kPR47uxGqmQNIQkIZr2_Tg</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Wanted to try this place for a while and it di...</td>\n",
       "      <td>2015-07-17 22:52:49</td>\n",
       "      <td>{timers, was, to, lava, disappoint, Gawd, 3, M...</td>\n",
       "      <td>{5, lava, to, disappoint, Gawd, 3, Marie, Oh, ...</td>\n",
       "      <td>[5, lava, disappoint, gawd, 3, marie, oh, brea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                review_id                 user_id             business_id  \\\n",
       "0  kJa4M0WsQaGwoMDSP0c-hw  sizanx4RIwRC3BfhQv8KOg  kPR47uxGqmQNIQkIZr2_Tg   \n",
       "1  8qst2v9HE8tV7B3IFvtSIA  DXP3K7wG_nu9jYPzQ8xVFA  qHQPvp6pZ75fB63kOKUPqg   \n",
       "2  DYAIyC476YkouqJgXuUv_A  OTRxfMCtfrbUemIigxCaOg  vhIJ91MDgUuk4Cr9Kpj1Nw   \n",
       "3  vCJS9ok_G3OfuxCtT3UNYg  d84GXn6uhT1qyDEziAmR8Q  oH4iqq4kjJfXpHCgB9G1sw   \n",
       "4  rMpvc3Hua7qRtDph8_pnpw  Ixaz7mLwzGQGg_W2yfBGqg  kPR47uxGqmQNIQkIZr2_Tg   \n",
       "\n",
       "   stars  useful  funny  cool  \\\n",
       "0      5       1      0     0   \n",
       "1      5       2      1     0   \n",
       "2      4       1      0     0   \n",
       "3      4       0      0     0   \n",
       "4      5       0      0     0   \n",
       "\n",
       "                                                text                date  \\\n",
       "0  Friendly staff and generous portions of great ... 2016-06-12 04:28:37   \n",
       "1  Job well done!! Thank you Rick for making our ... 2014-08-13 19:10:43   \n",
       "2  I'm a big fan of Jimmy John's. This location i... 2012-04-29 10:58:50   \n",
       "3  Well Spa was surprisingly posh and I loved it!... 2013-11-14 21:10:14   \n",
       "4  Wanted to try this place for a while and it di... 2015-07-17 22:52:49   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  {and, generous, eat, !, can, portions, ., u, g...   \n",
       "1  {look, extremely, was, to, excellence, Job, ar...   \n",
       "2  {rush, 's, mile, the, bacon, order, service, a...   \n",
       "3  {was, to, the, full, any, Spa, Great, loungers...   \n",
       "4  {timers, was, to, lava, disappoint, Gawd, 3, M...   \n",
       "\n",
       "                                          lemmatized  \\\n",
       "0  {portion, staff, and, of, generous, eat, can, ...   \n",
       "1  {look, extremely, to, excellence, Job, arrived...   \n",
       "2  {rush, 's, mile, bacon, the, doe, order, servi...   \n",
       "3  {to, the, full, any, Spa, Great, had, most, ma...   \n",
       "4  {5, lava, to, disappoint, Gawd, 3, Marie, Oh, ...   \n",
       "\n",
       "                                           processed  \n",
       "0  [portion, staff, generous, eat, u, great, fish...  \n",
       "1  [look, extremely, excellence, job, arrived, st...  \n",
       "2  [rush, mile, bacon, doe, order, service, alway...  \n",
       "3  [full, spa, great, massage, nice, lounger, 've...  \n",
       "4  [5, lava, disappoint, gawd, 3, marie, oh, brea...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# tokenizer: tokenize the text. break text into sentences and sentences into words\n",
    "reviews['tokens'] = reviews['text'].map(word_tokenize)\n",
    "reviews['tokens'] = reviews['tokens'].map(lambda x: set(x))\n",
    "\n",
    "# # lemmatizer: reduce words to their base form\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "reviews['lemmatized'] = reviews['tokens'].map(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "reviews['lemmatized'] = reviews['lemmatized'].map(lambda x: set(x))\n",
    "\n",
    "# # remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "reviews['processed'] = reviews['lemmatized'].map(lambda x: [word for word in x \n",
    "                                                            if word.lower() not in stop_words])\n",
    "\n",
    "# remove punctuations\n",
    "punc = \"!\\\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~...\"\n",
    "reviews['processed'] = reviews['processed'].map(lambda x: [word for word in x \n",
    "                                                           if word.lower() not in punc])\n",
    "\n",
    "# # remove some other stuff and return lower case\n",
    "others = [\"''\", \"``\", \"n't\", \"l\", \"'m\", \"'s\"]\n",
    "reviews['processed'] = reviews['processed'].map(lambda x: [word.lower() for word in x \n",
    "                                                 if word.lower() not in others])\n",
    "\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_reviews = reviews['processed'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Phrases\n",
    "bigram = Phrases(processed_reviews, min_count=50)\n",
    "for idx in range(len(processed_reviews)):\n",
    "    for token in bigram[processed_reviews[idx]]:\n",
    "        if '_' in token:\n",
    "            processed_reviews[idx].append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>processed</th>\n",
       "      <th>stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>[portion, staff, generous, eat, u, great, fish...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>[look, extremely, excellence, job, arrived, st...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>[rush, mile, bacon, doe, order, service, alway...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>[full, spa, great, massage, nice, lounger, 've...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>[5, lava, disappoint, gawd, 3, marie, oh, brea...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10224</td>\n",
       "      <td>[noted, service, definite, stopping, amount, f...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10225</td>\n",
       "      <td>[wo, due, size, problem, 9, restaurant, made, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10226</td>\n",
       "      <td>[fresh, sushi, close, bit, 4.5, restaurant, la...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10227</td>\n",
       "      <td>[decorating, grilled, 10, top, sign, order, al...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10228</td>\n",
       "      <td>[following, lined, highly, line, really, took,...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10229 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               processed  stars\n",
       "0      [portion, staff, generous, eat, u, great, fish...      5\n",
       "1      [look, extremely, excellence, job, arrived, st...      5\n",
       "2      [rush, mile, bacon, doe, order, service, alway...      4\n",
       "3      [full, spa, great, massage, nice, lounger, 've...      4\n",
       "4      [5, lava, disappoint, gawd, 3, marie, oh, brea...      5\n",
       "...                                                  ...    ...\n",
       "10224  [noted, service, definite, stopping, amount, f...      2\n",
       "10225  [wo, due, size, problem, 9, restaurant, made, ...      1\n",
       "10226  [fresh, sushi, close, bit, 4.5, restaurant, la...      4\n",
       "10227  [decorating, grilled, 10, top, sign, order, al...      4\n",
       "10228  [following, lined, highly, line, really, took,...      5\n",
       "\n",
       "[10229 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ReviewswStars = pd.DataFrame(processed_reviews)\n",
    "ReviewswStars['stars'] = reviews['stars']\n",
    "ReviewswStars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from gensim import corpora, models\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "Train_X, Test_X, Train_Y, Test_Y = model_selection.train_test_split(ReviewswStars['processed'], ReviewswStars['stars'], test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_X = Train_X.reset_index()\n",
    "Train_X = Train_X['processed']\n",
    "Train_Y = Train_Y.reset_index()\n",
    "Train_Y = Train_Y['stars']\n",
    "\n",
    "Test_X = Test_X.reset_index()\n",
    "Test_X = Test_X['processed']\n",
    "Test_Y = Test_Y.reset_index()\n",
    "Test_Y = Test_Y['stars']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\achen\\Anaconda3\\envs\\p36workshop\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "RandomForest = RandomForestRegressor()\n",
    "ModelTable = pd.DataFrame(columns = ['BelowThreshold', 'AboveThreshold', 'Topics', 'Passes', 'Workers','Score'])\n",
    "p=0\n",
    "\n",
    "##These are verified through X-validation\n",
    "\n",
    "belows = [20]\n",
    "aboves = [.4]\n",
    "numtop = [20]\n",
    "numpasses = [20]\n",
    "numworkers = [3]\n",
    "\n",
    "dictionary_train = gensim.corpora.Dictionary(Train_X)\n",
    "dictionary_test = gensim.corpora.Dictionary(Test_X)\n",
    "\n",
    "for belowtest in belows:\n",
    "    \n",
    "    for abovetest in aboves:\n",
    "        \n",
    "        dictionary_train.filter_extremes(no_below=belowtest, no_above=abovetest)\n",
    "        train_corpus = [dictionary_train.doc2bow(review) for review in Train_X]\n",
    "        \n",
    "        dictionary_test.filter_extremes(no_below=belowtest, no_above=abovetest)\n",
    "        test_corpus = [dictionary_test.doc2bow(review) for review in Test_X]\n",
    "        \n",
    "\n",
    "        for top in numtop:\n",
    "            \n",
    "            X_Train_WProbs = pd.DataFrame(Train_X.copy())\n",
    "            X_Test_WProbs = pd.DataFrame(Test_X.copy())\n",
    "            \n",
    "            for i in range(0,top):\n",
    "                \n",
    "                newcol = \"Topic_{}\".format(i)\n",
    "                \n",
    "                X_Train_WProbs.insert(1,column = newcol, value = 0)\n",
    "                \n",
    "                X_Test_WProbs.insert(1,column = newcol, value = 0)\n",
    "                \n",
    "            for numpass in numpasses:\n",
    "                \n",
    "                for work in numworkers:\n",
    "                    \n",
    "                    lda_model = gensim.models.LdaMulticore(train_corpus, num_topics = top, id2word=dictionary_train, passes = numpass, workers = work)\n",
    "                \n",
    "                    lda_model.save('lda_train.model')\n",
    "                    train_vecs = []\n",
    "                    for i in range(len(Train_X)):\n",
    "                        top_topics_train = lda_model.get_document_topics(train_corpus[i], minimum_probability=0.0)\n",
    "                        topic_vec_train = [top_topics_train[i][1] for i in range(0,top)]\n",
    "                        train_vecs.append(topic_vec_train)\n",
    "                    \n",
    "                    test_vecs = []\n",
    "                    for i in range(len(Test_X)):\n",
    "                        top_topics_test = lda_model.get_document_topics(test_corpus[i], minimum_probability=0.0)\n",
    "                        topic_vec_test = [top_topics_test[i][1] for i in range(0,top)]\n",
    "                        test_vecs.append(topic_vec_test)\n",
    "                        \n",
    "            \n",
    "#                     ModelTable.append(ModelRow)\n",
    "                    for n in range(0,len(train_vecs)):\n",
    "                        vec = train_vecs[n]\n",
    "                        for i in range(0,len(vec)):\n",
    "                            ColTitle = \"Topic_{}\".format(i)\n",
    "                            X_Train_WProbs.loc[n,ColTitle] = vec[i]\n",
    "\n",
    "\n",
    "                    for n in range(0,len(test_vecs)):\n",
    "                        vec = test_vecs[n]\n",
    "                        for i in range(0,len(vec)):\n",
    "                            ColTitle = \"Topic_{}\".format(i)\n",
    "                            X_Test_WProbs.loc[n,ColTitle] = vec[i]\n",
    "\n",
    "                    if p==0:\n",
    "                        X_train_regression = X_Train_WProbs.drop('processed', axis = 1, inplace = False)\n",
    "                        X_test_regression = X_Test_WProbs.drop('processed', axis = 1, inplace = False)\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "RandFor = RandomForest.fit(X_train_regression, Train_Y)\n",
    "Preds = RandFor.predict(X_test_regression)\n",
    "Score = np.mean((Preds-Test_Y)**2)\n",
    "ModelTable.loc[p,['BelowThreshold', 'AboveThreshold', 'Topics', 'Passes', 'Workers','Score']] = [belowtest, abovetest, top,numpass,work,Score]\n",
    "p +=1\n",
    "        \n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic_19</th>\n",
       "      <th>Topic_18</th>\n",
       "      <th>Topic_17</th>\n",
       "      <th>Topic_16</th>\n",
       "      <th>Topic_15</th>\n",
       "      <th>Topic_14</th>\n",
       "      <th>Topic_13</th>\n",
       "      <th>Topic_12</th>\n",
       "      <th>Topic_11</th>\n",
       "      <th>Topic_10</th>\n",
       "      <th>Topic_9</th>\n",
       "      <th>Topic_8</th>\n",
       "      <th>Topic_7</th>\n",
       "      <th>Topic_6</th>\n",
       "      <th>Topic_5</th>\n",
       "      <th>Topic_4</th>\n",
       "      <th>Topic_3</th>\n",
       "      <th>Topic_2</th>\n",
       "      <th>Topic_1</th>\n",
       "      <th>Topic_0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.002942</td>\n",
       "      <td>0.373354</td>\n",
       "      <td>0.002942</td>\n",
       "      <td>0.002942</td>\n",
       "      <td>0.002942</td>\n",
       "      <td>0.002942</td>\n",
       "      <td>0.002942</td>\n",
       "      <td>0.002942</td>\n",
       "      <td>0.225438</td>\n",
       "      <td>0.002942</td>\n",
       "      <td>0.002942</td>\n",
       "      <td>0.002942</td>\n",
       "      <td>0.002942</td>\n",
       "      <td>0.002942</td>\n",
       "      <td>0.002942</td>\n",
       "      <td>0.002942</td>\n",
       "      <td>0.002942</td>\n",
       "      <td>0.002942</td>\n",
       "      <td>0.002942</td>\n",
       "      <td>0.351200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000746</td>\n",
       "      <td>0.000746</td>\n",
       "      <td>0.084951</td>\n",
       "      <td>0.277048</td>\n",
       "      <td>0.000746</td>\n",
       "      <td>0.000746</td>\n",
       "      <td>0.000746</td>\n",
       "      <td>0.000746</td>\n",
       "      <td>0.004708</td>\n",
       "      <td>0.000746</td>\n",
       "      <td>0.000746</td>\n",
       "      <td>0.048735</td>\n",
       "      <td>0.159725</td>\n",
       "      <td>0.000746</td>\n",
       "      <td>0.056187</td>\n",
       "      <td>0.358942</td>\n",
       "      <td>0.000746</td>\n",
       "      <td>0.000746</td>\n",
       "      <td>0.000746</td>\n",
       "      <td>0.000746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.383542</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.210985</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.362966</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.002500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.152021</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.273625</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.415060</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.127290</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.005556</td>\n",
       "      <td>0.005556</td>\n",
       "      <td>0.493148</td>\n",
       "      <td>0.204790</td>\n",
       "      <td>0.005556</td>\n",
       "      <td>0.005556</td>\n",
       "      <td>0.005556</td>\n",
       "      <td>0.207609</td>\n",
       "      <td>0.005556</td>\n",
       "      <td>0.005556</td>\n",
       "      <td>0.005556</td>\n",
       "      <td>0.005556</td>\n",
       "      <td>0.005556</td>\n",
       "      <td>0.005556</td>\n",
       "      <td>0.005556</td>\n",
       "      <td>0.005556</td>\n",
       "      <td>0.005556</td>\n",
       "      <td>0.005556</td>\n",
       "      <td>0.005556</td>\n",
       "      <td>0.005556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8178</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.090926</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.170343</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.496692</td>\n",
       "      <td>0.048693</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.178341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8179</td>\n",
       "      <td>0.001725</td>\n",
       "      <td>0.001725</td>\n",
       "      <td>0.001725</td>\n",
       "      <td>0.001725</td>\n",
       "      <td>0.001725</td>\n",
       "      <td>0.001725</td>\n",
       "      <td>0.105495</td>\n",
       "      <td>0.001725</td>\n",
       "      <td>0.057552</td>\n",
       "      <td>0.001725</td>\n",
       "      <td>0.001725</td>\n",
       "      <td>0.001725</td>\n",
       "      <td>0.001725</td>\n",
       "      <td>0.001725</td>\n",
       "      <td>0.001725</td>\n",
       "      <td>0.221645</td>\n",
       "      <td>0.271901</td>\n",
       "      <td>0.001725</td>\n",
       "      <td>0.317537</td>\n",
       "      <td>0.001725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8180</td>\n",
       "      <td>0.001352</td>\n",
       "      <td>0.001352</td>\n",
       "      <td>0.455286</td>\n",
       "      <td>0.001352</td>\n",
       "      <td>0.001352</td>\n",
       "      <td>0.001352</td>\n",
       "      <td>0.001352</td>\n",
       "      <td>0.001352</td>\n",
       "      <td>0.001352</td>\n",
       "      <td>0.520384</td>\n",
       "      <td>0.001352</td>\n",
       "      <td>0.001352</td>\n",
       "      <td>0.001352</td>\n",
       "      <td>0.001352</td>\n",
       "      <td>0.001352</td>\n",
       "      <td>0.001352</td>\n",
       "      <td>0.001352</td>\n",
       "      <td>0.001352</td>\n",
       "      <td>0.001352</td>\n",
       "      <td>0.001352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8181</td>\n",
       "      <td>0.002382</td>\n",
       "      <td>0.002382</td>\n",
       "      <td>0.002382</td>\n",
       "      <td>0.002382</td>\n",
       "      <td>0.113832</td>\n",
       "      <td>0.002382</td>\n",
       "      <td>0.002382</td>\n",
       "      <td>0.002382</td>\n",
       "      <td>0.002382</td>\n",
       "      <td>0.002382</td>\n",
       "      <td>0.002382</td>\n",
       "      <td>0.258671</td>\n",
       "      <td>0.002382</td>\n",
       "      <td>0.002382</td>\n",
       "      <td>0.002382</td>\n",
       "      <td>0.002382</td>\n",
       "      <td>0.002382</td>\n",
       "      <td>0.002382</td>\n",
       "      <td>0.587012</td>\n",
       "      <td>0.002382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8182</td>\n",
       "      <td>0.001316</td>\n",
       "      <td>0.001316</td>\n",
       "      <td>0.001316</td>\n",
       "      <td>0.001316</td>\n",
       "      <td>0.001316</td>\n",
       "      <td>0.001316</td>\n",
       "      <td>0.001316</td>\n",
       "      <td>0.404060</td>\n",
       "      <td>0.001316</td>\n",
       "      <td>0.001316</td>\n",
       "      <td>0.001316</td>\n",
       "      <td>0.132703</td>\n",
       "      <td>0.001316</td>\n",
       "      <td>0.001316</td>\n",
       "      <td>0.001316</td>\n",
       "      <td>0.001316</td>\n",
       "      <td>0.001316</td>\n",
       "      <td>0.001316</td>\n",
       "      <td>0.440863</td>\n",
       "      <td>0.001316</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8183 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Topic_19  Topic_18  Topic_17  Topic_16  Topic_15  Topic_14  Topic_13  \\\n",
       "0     0.002942  0.373354  0.002942  0.002942  0.002942  0.002942  0.002942   \n",
       "1     0.000746  0.000746  0.084951  0.277048  0.000746  0.000746  0.000746   \n",
       "2     0.002500  0.002500  0.383542  0.002500  0.002500  0.210985  0.002500   \n",
       "3     0.002000  0.002000  0.152021  0.002000  0.002000  0.002000  0.002000   \n",
       "4     0.005556  0.005556  0.493148  0.204790  0.005556  0.005556  0.005556   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "8178  0.001000  0.001000  0.001000  0.001000  0.001000  0.001000  0.001000   \n",
       "8179  0.001725  0.001725  0.001725  0.001725  0.001725  0.001725  0.105495   \n",
       "8180  0.001352  0.001352  0.455286  0.001352  0.001352  0.001352  0.001352   \n",
       "8181  0.002382  0.002382  0.002382  0.002382  0.113832  0.002382  0.002382   \n",
       "8182  0.001316  0.001316  0.001316  0.001316  0.001316  0.001316  0.001316   \n",
       "\n",
       "      Topic_12  Topic_11  Topic_10   Topic_9   Topic_8   Topic_7   Topic_6  \\\n",
       "0     0.002942  0.225438  0.002942  0.002942  0.002942  0.002942  0.002942   \n",
       "1     0.000746  0.004708  0.000746  0.000746  0.048735  0.159725  0.000746   \n",
       "2     0.002500  0.002500  0.002500  0.362966  0.002500  0.002500  0.002500   \n",
       "3     0.002000  0.273625  0.002000  0.002000  0.002000  0.002000  0.002000   \n",
       "4     0.207609  0.005556  0.005556  0.005556  0.005556  0.005556  0.005556   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "8178  0.001000  0.001000  0.090926  0.001000  0.001000  0.170343  0.001000   \n",
       "8179  0.001725  0.057552  0.001725  0.001725  0.001725  0.001725  0.001725   \n",
       "8180  0.001352  0.001352  0.520384  0.001352  0.001352  0.001352  0.001352   \n",
       "8181  0.002382  0.002382  0.002382  0.002382  0.258671  0.002382  0.002382   \n",
       "8182  0.404060  0.001316  0.001316  0.001316  0.132703  0.001316  0.001316   \n",
       "\n",
       "       Topic_5   Topic_4   Topic_3   Topic_2   Topic_1   Topic_0  \n",
       "0     0.002942  0.002942  0.002942  0.002942  0.002942  0.351200  \n",
       "1     0.056187  0.358942  0.000746  0.000746  0.000746  0.000746  \n",
       "2     0.002500  0.002500  0.002500  0.002500  0.002500  0.002500  \n",
       "3     0.002000  0.415060  0.002000  0.002000  0.127290  0.002000  \n",
       "4     0.005556  0.005556  0.005556  0.005556  0.005556  0.005556  \n",
       "...        ...       ...       ...       ...       ...       ...  \n",
       "8178  0.001000  0.001000  0.496692  0.048693  0.001000  0.178341  \n",
       "8179  0.001725  0.221645  0.271901  0.001725  0.317537  0.001725  \n",
       "8180  0.001352  0.001352  0.001352  0.001352  0.001352  0.001352  \n",
       "8181  0.002382  0.002382  0.002382  0.002382  0.587012  0.002382  \n",
       "8182  0.001316  0.001316  0.001316  0.001316  0.440863  0.001316  \n",
       "\n",
       "[8183 rows x 20 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "model = RandomForestRegressor()\n",
    "\n",
    "parameters = {\n",
    "    \"n_estimators\": [50, 90],\n",
    "    \"max_depth\":[10, 20],\n",
    "    'min_samples_split': [50, 100],\n",
    "    'max_features': ['sqrt']\n",
    "     }\n",
    "\n",
    "RF_clf = GridSearchCV(model,\n",
    "                    parameters, n_jobs=-1,\n",
    "                    scoring=\"r2\",\n",
    "                    cv=3, verbose = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  24 | elapsed:    3.5s remaining:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:    4.5s finished\n"
     ]
    }
   ],
   "source": [
    "RF_fit = RF_clf.fit(X_train_regression, Train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score obtained: 0.372\n",
      "Best parameters found:\n",
      " {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_split': 50, 'n_estimators': 90}\n"
     ]
    }
   ],
   "source": [
    "print('Best score obtained: %0.3f' % RF_fit.best_score_)\n",
    "print('Best parameters found:\\n', RF_fit.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_best = RandomForestRegressor(max_depth=20, max_features='sqrt', min_samples_split=50, \n",
    "                                n_estimators = 90)\n",
    "RF_best.fit(X_train_regression, Train_Y)\n",
    "y_pred = RF_best.predict(X_test_regression)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "At this point I think some more things that could be done:\n",
    "\n",
    "1) LDA with TF-IDF rather than bag of words\n",
    "\n",
    "2) Using either LDA (TF-IDF/BoW) with different types of regression models\n",
    "\n",
    "3) Cop out answer: BETTER PREPROCESSING XDDDDDDDDDDDDDDDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
